{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa56126f-f254-4fd7-852d-1055cfa8fb84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:17:45.966579Z",
     "start_time": "2025-03-06T05:17:45.943797Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-06T10:40:41.660199Z",
     "iopub.status.busy": "2025-03-06T10:40:41.659756Z",
     "iopub.status.idle": "2025-03-06T10:40:41.682361Z",
     "shell.execute_reply": "2025-03-06T10:40:41.681770Z",
     "shell.execute_reply.started": "2025-03-06T10:40:41.660178Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from typing import Dict, Any\n",
    "import json\n",
    "import uuid\n",
    "from functools import partial\n",
    "from typing import Iterable, Dict, Any\n",
    "# create spark session\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from loguru import logger\n",
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "from llm_web_kit.extractor.extractor_chain import ExtractSimpleFactory\n",
    "from xinghe.s3 import *\n",
    "\n",
    "\n",
    "import uuid\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from llm_web_kit.input.datajson import DataJson\n",
    "from func_timeout import FunctionTimedOut, func_timeout\n",
    "\n",
    "#| track_id                  | uuid                           | 全局唯一的ID                                                                                                        | 是              |\n",
    "#| dataset_name              | str                            | 数据集的名字（全局唯一），这个名字是管理员输入的，然后做索引的时候带到index里来                                     | 是              |\n",
    "#| data_source_category      | str                            | 这一行数据代表的是HTML，PDF，EBOOK,CC,labCC类型                                                                     | 是，此处是 HTML |\n",
    "#| html                      | 字符串                         | 以UTF-8为编码的HTML文件全文                                                                                         | 是              |\n",
    "#| url                       | 字符串                         | 这个文件的来源网址                                                                                                  | 是              |\n",
    "#| file_bytes                | 整数                           | 文件的size, 单位是byte                                                                                              | 是              |\n",
    "#| meta_info                 | 字典                           | 存放关于文件的元信息:如果能从文件里获取到作者，制作日期等信息。或者数据本身就带有一些其他的信息都放入到这个字段里。 | 是              |\n",
    "#| meta_info->input_datetime | 其格式为 `yyyy-mm-dd HH:MM:SS` | 生成这个json索引文件这一条数据的时间，可以不用那么精确                                                              | 是              |\n",
    "\n",
    "\n",
    "def process_platform_data(spark: SparkSession, config,platform: str, input_paths: list, version: str = \"001\"):\n",
    "    \"\"\"接收 spark 作为参数而不是持有它\"\"\"\n",
    "    platform_config = platform_configs.get(platform)\n",
    "    if not platform_config:\n",
    "        raise ValueError(f\"Unsupported platform: {platform}\")\n",
    "\n",
    "    # Driver 端操作\n",
    "    input_df = read_any_path(spark, \",\".join(input_paths), config)\n",
    "    print(f\"读取数据结束\")\n",
    "    # 准备 Worker 端配置\n",
    "    worker_config = {\n",
    "        \"field_mappings\": platform_config.get(\"field_mappings\"),\n",
    "        \"extractor_config\": platform_config.get(\"extractor_config\")\n",
    "    }\n",
    "    # 分别广播不同配置\n",
    "    broadcast_field_mappings = spark.sparkContext.broadcast(worker_config[\"field_mappings\"])\n",
    "    broadcast_extractor_config_path = spark.sparkContext.broadcast(worker_config[\"extractor_config\"])  # 广播路径\n",
    "\n",
    "    print(\"原始数据json结构\")\n",
    "    input_rdd = input_df.rdd.map(lambda x: Row(**{**json.loads(x.value), \"filename\": x.filename})).cache()\n",
    "  #  pandas_df = input_rdd.toDF().toPandas()\n",
    "  #  print(pandas_df)\n",
    "    print(input_rdd.take(1)[0].asDict().keys())\n",
    "    # 数据转换（仅用字段映射）\n",
    "    transformed_rdd = input_rdd.map(\n",
    "        lambda row: transform_row(row, broadcast_field_mappings.value)\n",
    "    )\n",
    "   \n",
    "    print(\"formatter数据结束\")\n",
    "    # 数据抽取（仅用抽取器配置路径）\n",
    "    processed_rdd = transformed_rdd.mapPartitions(\n",
    "         lambda x:extract_data(\n",
    "            x,broadcast_extractor_config_path = broadcast_extractor_config_path.value\n",
    "        )\n",
    "    )\n",
    "    print(\"extractor数据结束\")\n",
    "    print(\"写入数据中\")\n",
    "   \n",
    "    # 输出结果Row(value=json.dumps(x.asDict()))).toDF()\n",
    "    write_any_path(processed_rdd.map(lambda x: Row(value=json.dumps(x.asDict()))).toDF(), platform_config[\"output_template\"],config)\n",
    "    print(\"写入数据结束\")\n",
    "\n",
    "    # 清理广播变量\n",
    "    broadcast_field_mappings.unpersist()\n",
    "    broadcast_extractor_config_path.unpersist()\n",
    "\n",
    "\n",
    "\n",
    "def transform_row(row, config: dict) -> Row:\n",
    "    \"\"\"根据平台配置转换行数据\"\"\"\n",
    "    mappings = config\n",
    "\n",
    "    return Row(\n",
    "        track_id=getattr(row, mappings[\"track_id\"],str(uuid.uuid4())),\n",
    "        url=getattr(row, mappings[\"url\"], ''),\n",
    "        html=getattr(row, mappings[\"html\"], ''),\n",
    "        page_layout_type=mappings.get(\"page_layout_type_map\").get(\n",
    "            getattr(row, mappings[\"layout_field\"], ''),\n",
    "            \"article\"\n",
    "        ),\n",
    "        domain=extract_domain_info(getattr(row, mappings[\"url\"], ''))['domain'],\n",
    "        dataset_name=mappings[\"dataset_name\"],\n",
    "        data_source_category=mappings[\"data_source_category\"],\n",
    "        meta_info={\"filename\":row.filename}\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_data(partition, broadcast_extractor_config_path):\n",
    "    from loguru import logger\n",
    "    extractor_chain = ExtractSimpleFactory.create('/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc')\n",
    "    timeout_seconds = 10\n",
    "    # 为每个分区创建唯一的错误日志文件\n",
    "    # partition_id = str(uuid.uuid4())\n",
    "    # current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # error_log_path = f\"s3://xyz-llm-users/xyz-users/yujia/CC-MAIN-2024-33/output/v002/error_logs/{current_time}_{partition_id}.json\"\n",
    "    # s3_doc_writer = S3DocWriter(path=error_log_path)\n",
    "    try:\n",
    "        for row in partition:\n",
    "            d = row.asDict()\n",
    "           \n",
    "            input_data = DataJson(d)\n",
    "            data_e: DataJson = func_timeout(timeout_seconds, extractor_chain.extract, args=(input_data,))\n",
    "            #data_e: DataJson = extractor_chain.extract(input_data)\n",
    "            yield Row(**data_e.to_dict())\n",
    "    except FunctionTimedOut as e1:\n",
    "            d['__error'] = {\n",
    "                \"error_type\":\"TIMEOUT\",\n",
    "                \"error_message\": \"extract function timeout\",\n",
    "                \"traceback\":\"TIMEOUT\"\n",
    "            }\n",
    "            yield Row(**d)\n",
    "    except Exception as e:\n",
    "            # 记录更详细的错误信息\n",
    "            error_info = {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"traceback\": traceback.format_exc(),\n",
    "            }\n",
    "            logger.error(error_info)\n",
    "            # s3_doc_writer.write(error_info)\n",
    "            d['__error'] = error_info\n",
    "            yield Row(**d)\n",
    "           \n",
    "\n",
    "\n",
    "def _safe_extract(data: Dict, extractor, timeout: int = 10) -> Dict:\n",
    "  \n",
    "    \"\"\"直接返回字典，避免生成器\"\"\"\n",
    "    try:\n",
    "        timeout_seconds = 10\n",
    "        input_data = DataJson(data.asDict())\n",
    "        print(input_data)\n",
    "        data_e: DataJson = func_timeout(timeout_seconds, extractor.extract, \\\n",
    "                                                args=(input_data,))\n",
    "\n",
    "        print(Row(**data_e.to_dict()))\n",
    "        return data_e.to_dict() \n",
    "    except Exception as e:\n",
    "        # 记录更详细的错误信息\n",
    "        error_info = {\n",
    "            \"error_type\": type(e).__name__,\n",
    "            \"error_message\": str(e),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "        }\n",
    "        logger.error(error_info)\n",
    "        # s3_doc_writer.write(error_info)\n",
    "        data['__error'] = error_info\n",
    "        yield Row(**d)\n",
    "\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def extract_domain_info(url: str) -> dict:\n",
    "    \"\"\"从 URL 中提取完整的域名信息\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    netloc = parsed.netloc\n",
    "    domain_parts = netloc.split(\":\")\n",
    "    domain = domain_parts[0]  # 去除端口号\n",
    "    root_domain = \".\".join(domain.split(\".\")[-2:]) if len(domain.split(\".\")) >= 2 else domain\n",
    "\n",
    "    return {\n",
    "        \"full_url\": url,\n",
    "        \"netloc\": netloc,\n",
    "        \"domain\": domain,\n",
    "        \"root_domain\": root_domain\n",
    "    }\n",
    "        \n",
    "def handle_error(row: Dict, error: Exception) -> Dict:\n",
    "    \"\"\"统一错误处理\"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    return {\n",
    "     ** row,\n",
    "    \"__error\": {\n",
    "        \"type\": type(error).__name__,\n",
    "        \"message\": str(error),\n",
    "        \"traceback\": traceback.format_exc()\n",
    "    }\n",
    "    }\n",
    "\n",
    "    \n",
    "def extract_platform_from_s3_path(s3_path: str) -> str:\n",
    "    \"\"\"\n",
    "    从 S3 路径中提取平台名称（存储桶后的第一个目录）\n",
    "    \n",
    "    示例输入: \n",
    "    - \"s3://private-cooperate-data/zh-web-baijiahao/20241218_p1/\"\n",
    "    输出: \"zh-web-baijiahao\"\n",
    "    \n",
    "    - \"s3://private-cooperate-data/DouBan/\"\n",
    "    输出: \"DouBan\"\n",
    "    \"\"\"\n",
    "    # 分割路径并过滤空字符串\n",
    "    parts = [p for p in s3_path.split(\"/\") if p.strip() != \"\"]\n",
    "    \n",
    "    # 验证路径格式\n",
    "    if len(parts) < 3:\n",
    "        raise ValueError(f\"无效的 S3 路径格式: {s3_path}\")\n",
    "    \n",
    "    # 平台名称是存储桶后的第一个目录\n",
    "    return parts[2]\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_4\",\n",
    "    \"skip_success_check\": True,\n",
    "    \"spark.executor.memory\":\"8g\",\n",
    "    # 根据个人路径进行替换1\n",
    "    \"spark.executorEnv.LLM_WEB_KIT_CFG_PATH\": \"/share/jiangwenhao/.llm-web-kit.jsonc\",\n",
    "    \"spark.dynamicAllocation.maxExecutors\": \"400\"\n",
    "}\n",
    "paths = [\n",
    "#\"s3://private-cooperate-data/zh-web-baijiahao/\",\n",
    "#\"s3://private-crawl-data/zh-web-netease/20241218_p1/\",\n",
    "   \"s3://private-crawl-data/zh-web-tencent/20241218_p1/\",\n",
    "    \"s3://private-crawl-data/zh-web-sohu/20241218_p1/\",\n",
    " #   \"s3://private-crawl-data/zh-web-sina/20241218_p1/\",\n",
    "   # \"s3://crawl-data/blog_sina_com_cn/gz_file/1729501052/\",\n",
    "   # \"s3://private-cooperate-data/DouBan/\"\n",
    "    \n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# spark = new_spark_session(\"llm_kit_cc\", config)\n",
    "# version=\"008\"\n",
    "# for path in paths:\n",
    "#     platform = extract_platform_from_s3_path(path)\n",
    "#     print(f\"路径: {path} → 平台: {platform}\")\n",
    "#     platform_configs = {\n",
    "#\n",
    "#     \"zh-web-baijiahao\": {\n",
    "#\n",
    "#         \"field_mappings\": {\n",
    "#             \"track_id\":\"track_id\",\n",
    "#             \"page_layout_type_map\" :{\n",
    "#                                 \"\":\"article\",\n",
    "#                                 \"文章\":\"article\",\n",
    "#                                 \"网易\":'article',\n",
    "#                                 \"视频\":\"video\"\n",
    "#                             },\n",
    "#         \"dataset_name\": \"baijiahao\",\n",
    "#         \"url\":\"url\",\n",
    "#         \"html\":\"content\",\n",
    "#         \"layout_field\": \"channel\",\n",
    "#         \"data_source_category\":\"JSON\"\n",
    "#         },\n",
    "#         \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "#         \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/v{version}/\"\n",
    "#\n",
    "#     },\n",
    "#     \"zh-web-netease\": {\n",
    "#\n",
    "#         \"field_mappings\": {\n",
    "#              \"track_id\":\"track_id\",\n",
    "#             \"page_layout_type_map\" :{\n",
    "#                                 \"\":\"article\",\n",
    "#                                 \"文章\":\"article\",\n",
    "#                                 \"网易\":'article',\n",
    "#                                 \"视频\":\"video\"\n",
    "#                             },\n",
    "#         \"dataset_name\": \"net-ease\",\n",
    "#         \"url\":\"url\",\n",
    "#         \"html\":\"content\",\n",
    "#         \"layout_field\": \"f_name\",\n",
    "#         \"data_source_category\":\"HTML\"\n",
    "#         },\n",
    "#         \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "#         \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/v{version}/\"\n",
    "#     },\n",
    "#     \"zh-web-tencent\": {\n",
    "#\n",
    "#         \"field_mappings\": {\n",
    "#              \"track_id\":\"track_id\",\n",
    "#             \"page_layout_type_map\" :{\n",
    "#                                 \"\":\"article\",\n",
    "#                                 \"文章\":\"article\",\n",
    "#                                 \"网易\":'article',\n",
    "#                                 \"视频\":\"video\",\n",
    "#                                 \"腾讯网\":\"article\"\n",
    "#                             },\n",
    "#         \"dataset_name\": \"tencent\",\n",
    "#         \"url\":\"url\",\n",
    "#         \"html\":\"content\",\n",
    "#         \"layout_field\": \"f_name\",\n",
    "#         \"data_source_category\":\"HTML\"\n",
    "#         },\n",
    "#         \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "#         \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/v{version}/\"\n",
    "#     },\n",
    "#      \"zh-web-sohu\": {\n",
    "#\n",
    "#         \"field_mappings\": {\n",
    "#              \"track_id\":\"track_id\",\n",
    "#             \"page_layout_type_map\" :{\n",
    "#                                 \"\":\"article\",\n",
    "#                                 \"文章\":\"article\",\n",
    "#                                 \"网易\":'article',\n",
    "#                                 \"视频\":\"video\",\n",
    "#                                 \"搜狐网\":\"article\"\n",
    "#                             },\n",
    "#         \"dataset_name\": \"souhu\",\n",
    "#         \"url\":\"url\",\n",
    "#         \"html\":\"content\",\n",
    "#         \"layout_field\": \"f_name\",\n",
    "#         \"data_source_category\":\"HTML\"\n",
    "#         },\n",
    "#         \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "#         \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/v{version}/\"\n",
    "#     },\n",
    "#     \"zh-web-sina\": {\n",
    "#\n",
    "#         \"field_mappings\": {\n",
    "#              \"track_id\":\"track_id\",\n",
    "#             \"page_layout_type_map\" :{\n",
    "#                                 \"\":\"article\",\n",
    "#                                 \"文章\":\"article\",\n",
    "#                                 \"网易\":'article',\n",
    "#                                 \"视频\":\"video\",\n",
    "#                                 \"搜狐网\":\"article\",\n",
    "#                                 \"黑猫投诉\":\"forum\"\n",
    "#                             },\n",
    "#         \"dataset_name\": \"sina\",\n",
    "#         \"url\":\"url\",\n",
    "#         \"html\":\"content\",\n",
    "#         \"layout_field\": \"f_name\",\n",
    "#         \"data_source_category\":\"HTML\"\n",
    "#         },\n",
    "#         \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "#         \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/v{version}/\"\n",
    "#     },\n",
    "#\n",
    "#     \"blog_sina_com_cn\": {\n",
    "#\n",
    "#         \"field_mappings\": {\n",
    "#              \"track_id\":\"track_id\",\n",
    "#             \"page_layout_type_map\" :{\n",
    "#                                 \"\":\"article\",\n",
    "#                                 \"文章\":\"article\",\n",
    "#                                 \"网易\":'article',\n",
    "#                                 \"视频\":\"video\",\n",
    "#                                 \"搜狐网\":\"article\",\n",
    "#                                 \"黑猫投诉\":\"forum\"\n",
    "#                             },\n",
    "#         \"dataset_name\": \"sina\",\n",
    "#         \"url\":\"url\",\n",
    "#         \"html\":\"html\",\n",
    "#         \"layout_field\": \"f_name\",\n",
    "#         \"data_source_category\":\"HTML\"\n",
    "#         },\n",
    "#         \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "#         \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/v{version}/\"\n",
    "#     },\n",
    "#     \"DouBan\": {\n",
    "#\n",
    "#         \"field_mappings\": {\n",
    "#              \"track_id\":\"track_id\",\n",
    "#             \"page_layout_type_map\" :{\n",
    "#                                 \"\":\"article\",\n",
    "#                                 \"文章\":\"article\",\n",
    "#                                 \"网易\":'article',\n",
    "#                                 \"视频\":\"video\",\n",
    "#                                 \"搜狐网\":\"article\",\n",
    "#                                 \"黑猫投诉\":\"forum\",\n",
    "#                                 \"豆瓣网\":\"\"\n",
    "#                             },\n",
    "#         \"dataset_name\": \"sina\",\n",
    "#         \"url\":\"url\",\n",
    "#         \"html\":\"html\",\n",
    "#         \"layout_field\": \"f_name\",\n",
    "#         \"data_source_category\":\"HTML\"\n",
    "#         },\n",
    "#         \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "#         \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/v{version}/\"\n",
    "#     },\n",
    "#}\n",
    "    # 处理正式数据\n",
    " #  process_platform_data(\n",
    " #      spark=spark,\n",
    " #      config=config,\n",
    " #      platform=platform,\n",
    " #      input_paths=[path],\n",
    " #      version=version\n",
    " # )\n",
    "\n",
    "\n",
    "    # 处理其他平台数据\n",
    "    # processor.process_platform_data(\"other_platform\", [...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4bc53ba-2a43-4130-887b-fcca71c9a24e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:17:47.066769Z",
     "start_time": "2025-03-06T05:17:47.062493Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-06T10:41:25.232712Z",
     "iopub.status.busy": "2025-03-06T10:41:25.232190Z",
     "iopub.status.idle": "2025-03-06T10:41:25.237082Z",
     "shell.execute_reply": "2025-03-06T10:41:25.236502Z",
     "shell.execute_reply.started": "2025-03-06T10:41:25.232692Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from typing import Dict, Any\n",
    "import json\n",
    "import uuid\n",
    "from functools import partial\n",
    "from typing import Iterable, Dict, Any\n",
    "# create spark session\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from loguru import logger\n",
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "\n",
    "from xinghe.s3 import *\n",
    "\n",
    "\n",
    "import uuid\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from llm_web_kit.input.datajson import DataJson\n",
    "from func_timeout import FunctionTimedOut, func_timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b86433d1431d91ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:24:19.490965Z",
     "start_time": "2025-03-06T05:24:19.478894Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-06T10:41:28.389124Z",
     "iopub.status.busy": "2025-03-06T10:41:28.388633Z",
     "iopub.status.idle": "2025-03-06T10:41:28.401712Z",
     "shell.execute_reply": "2025-03-06T10:41:28.401132Z",
     "shell.execute_reply.started": "2025-03-06T10:41:28.389104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'field_mappings': {'track_id': 'track_id',\n",
       "  'page_layout_type_map': {'': 'article',\n",
       "   '文章': 'article',\n",
       "   '网易': 'article',\n",
       "   '视频': 'video',\n",
       "   '腾讯网': 'article'},\n",
       "  'dataset_name': 'tencent',\n",
       "  'url': 'url',\n",
       "  'html': 'content',\n",
       "  'layout_field': 'f_name',\n",
       "  'data_source_category': 'HTML'},\n",
       " 'extractor_config': '/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc',\n",
       " 'output_template': 's3://llm-users-phdd2/jiangwenhao/article/zh-web-tencent/v018/'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version=\"018\"\n",
    "# sub_dir = \"202401\"\n",
    "input_paths =[f's3://private-crawl-data/zh-web-tencent/20241218_p1/']\n",
    "platform = extract_platform_from_s3_path(input_paths[0])\n",
    "platform_configs = {\n",
    "\n",
    "    \"zh-web-baijiahao\": {\n",
    "\n",
    "        \"field_mappings\": {\n",
    "            \"track_id\":\"track_id\",\n",
    "            \"page_layout_type_map\" :{\n",
    "                                \"\":\"article\",\n",
    "                                \"文章\":\"article\",\n",
    "                                \"网易\":'article',\n",
    "                                \"视频\":\"video\"\n",
    "                            },\n",
    "        \"dataset_name\": \"baijiahao\",\n",
    "        \"url\":\"url\",\n",
    "        \"html\":\"content\",\n",
    "        \"layout_field\": \"channel\",\n",
    "        \"data_source_category\":\"JSON\"\n",
    "        },\n",
    "        \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "        \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/{sub_dir}/v{version}/\"\n",
    "\n",
    "    },\n",
    "    \"zh-web-netease\": {\n",
    "\n",
    "        \"field_mappings\": {\n",
    "             \"track_id\":\"track_id\",\n",
    "            \"page_layout_type_map\" :{\n",
    "                                \"\":\"article\",\n",
    "                                \"文章\":\"article\",\n",
    "                                \"网易\":'article',\n",
    "                                \"视频\":\"video\"\n",
    "                            },\n",
    "        \"dataset_name\": \"net-ease\",\n",
    "        \"url\":\"url\",\n",
    "        \"html\":\"content\",\n",
    "        \"layout_field\": \"f_name\",\n",
    "        \"data_source_category\":\"HTML\"\n",
    "        },\n",
    "        \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "        \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/v{version}/\"\n",
    "    },\n",
    "    \"zh-web-tencent\": {\n",
    "\n",
    "        \"field_mappings\": {\n",
    "             \"track_id\":\"track_id\",\n",
    "            \"page_layout_type_map\" :{\n",
    "                                \"\":\"article\",\n",
    "                                \"文章\":\"article\",\n",
    "                                \"网易\":'article',\n",
    "                                \"视频\":\"video\",\n",
    "                                \"腾讯网\":\"article\"\n",
    "                            },\n",
    "        \"dataset_name\": \"tencent\",\n",
    "        \"url\":\"url\",\n",
    "        \"html\":\"content\",\n",
    "        \"layout_field\": \"f_name\",\n",
    "        \"data_source_category\":\"HTML\"\n",
    "        },\n",
    "        \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "        \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/v{version}/\"\n",
    "    },\n",
    "     \"zh-web-sohu\": {\n",
    "\n",
    "        \"field_mappings\": {\n",
    "             \"track_id\":\"track_id\",\n",
    "            \"page_layout_type_map\" :{\n",
    "                                \"\":\"article\",\n",
    "                                \"文章\":\"article\",\n",
    "                                \"网易\":'article',\n",
    "                                \"视频\":\"video\",\n",
    "                                \"搜狐网\":\"article\"\n",
    "                            },\n",
    "        \"dataset_name\": \"souhu\",\n",
    "        \"url\":\"url\",\n",
    "        \"html\":\"content\",\n",
    "        \"layout_field\": \"f_name\",\n",
    "        \"data_source_category\":\"HTML\"\n",
    "        },\n",
    "        \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "        \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/v{version}/\"\n",
    "    },\n",
    "    \"zh-web-sina\": {\n",
    "\n",
    "        \"field_mappings\": {\n",
    "             \"track_id\":\"track_id\",\n",
    "            \"page_layout_type_map\" :{\n",
    "                                \"\":\"article\",\n",
    "                                \"文章\":\"article\",\n",
    "                                \"网易\":'article',\n",
    "                                \"视频\":\"video\",\n",
    "                                \"搜狐网\":\"article\",\n",
    "                                \"黑猫投诉\":\"forum\"\n",
    "                            },\n",
    "        \"dataset_name\": \"sina\",\n",
    "        \"url\":\"url\",\n",
    "        \"html\":\"content\",\n",
    "        \"layout_field\": \"f_name\",\n",
    "        \"data_source_category\":\"HTML\"\n",
    "        },\n",
    "        \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "        \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/v{version}/\"\n",
    "    },\n",
    "\n",
    "    \"blog_sina_com_cn\": {\n",
    "\n",
    "        \"field_mappings\": {\n",
    "             \"track_id\":\"track_id\",\n",
    "            \"page_layout_type_map\" :{\n",
    "                                \"\":\"article\",\n",
    "                                \"文章\":\"article\",\n",
    "                                \"网易\":'article',\n",
    "                                \"视频\":\"video\",\n",
    "                                \"搜狐网\":\"article\",\n",
    "                                \"黑猫投诉\":\"forum\"\n",
    "                            },\n",
    "        \"dataset_name\": \"sina\",\n",
    "        \"url\":\"url\",\n",
    "        \"html\":\"html\",\n",
    "        \"layout_field\": \"f_name\",\n",
    "        \"data_source_category\":\"HTML\"\n",
    "        },\n",
    "        \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "        \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/v{version}/\"\n",
    "    },\n",
    "    \"DouBan\": {\n",
    "       \n",
    "        \"field_mappings\": {\n",
    "             \"track_id\":\"track_id\",\n",
    "            \"page_layout_type_map\" :{\n",
    "                                \"\":\"article\",\n",
    "                                \"文章\":\"article\",\n",
    "                                \"网易\":'article',\n",
    "                                \"视频\":\"video\",\n",
    "                                \"搜狐网\":\"article\",\n",
    "                                \"黑猫投诉\":\"forum\",\n",
    "                                \"豆瓣网\":\"\"\n",
    "                            },\n",
    "        \"dataset_name\": \"sina\",\n",
    "        \"url\":\"url\",\n",
    "        \"html\":\"html\",\n",
    "        \"layout_field\": \"f_name\",\n",
    "        \"data_source_category\":\"HTML\"\n",
    "        },\n",
    "        \"extractor_config\": \"/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc\",\n",
    "        \"output_template\": f\"s3://llm-users-phdd2/jiangwenhao/article/{platform}/v{version}/\"\n",
    "    },\n",
    "}\n",
    "platform_config = platform_configs.get(platform)\n",
    "platform_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ef2e435453b88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:24:20.573512Z",
     "start_time": "2025-03-06T05:24:20.571550Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206441bc2e5a5066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:24:21.150545Z",
     "start_time": "2025-03-06T05:24:21.148759Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b8b94ef49590564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:24:29.495664Z",
     "start_time": "2025-03-06T05:24:22.608403Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-06T10:41:29.821878Z",
     "iopub.status.busy": "2025-03-06T10:41:29.821506Z",
     "iopub.status.idle": "2025-03-06T10:41:40.251349Z",
     "shell.execute_reply": "2025-03-06T10:41:40.250525Z",
     "shell.execute_reply.started": "2025-03-06T10:41:29.821860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路径: s3://private-crawl-data/zh-web-tencent/20241218_p1/ → 平台: zh-web-tencent\n",
      "读取数据结束\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_4\",\n",
    "    \"skip_success_check\": True,\n",
    "    # 根据个人路径进行替换1\n",
    "    \"spark.executorEnv.LLM_WEB_KIT_CFG_PATH\": \"/share/jiangwenhao/.llm-web-kit.jsonc\",\n",
    "    \"spark.dynamicAllocation.maxExecutors\": \"400\",\n",
    "}\n",
    "spark = new_spark_session(\"llm_kit_cc\", config)\n",
    "\n",
    "\n",
    "print(f\"路径: {input_paths[0]} → 平台: {platform}\")\n",
    "\n",
    "\n",
    "\"\"\"接收 spark 作为参数而不是持有它\"\"\"\n",
    "platform_config = platform_configs.get(platform)\n",
    "if not platform_config:\n",
    "    raise ValueError(f\"Unsupported platform: {platform}\")\n",
    "\n",
    "# Driver 端操作\n",
    "input_df = read_any_path(spark, \",\".join(input_paths), config)\n",
    "print(f\"读取数据结束\")\n",
    "# 准备 Worker 端配置\n",
    "worker_config = {\n",
    "    \"field_mappings\": platform_config.get(\"field_mappings\"),\n",
    "    \"extractor_config\": platform_config.get(\"extractor_config\")\n",
    "}\n",
    "# 分别广播不同配置\n",
    "broadcast_field_mappings = spark.sparkContext.broadcast(worker_config[\"field_mappings\"])\n",
    "broadcast_extractor_config_path = spark.sparkContext.broadcast(worker_config[\"extractor_config\"])  # 广播路径\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35412d676d13b7f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:24:29.512690Z",
     "start_time": "2025-03-06T05:24:29.508671Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-06T10:41:40.252919Z",
     "iopub.status.busy": "2025-03-06T10:41:40.252618Z",
     "iopub.status.idle": "2025-03-06T10:41:40.257092Z",
     "shell.execute_reply": "2025-03-06T10:41:40.256533Z",
     "shell.execute_reply.started": "2025-03-06T10:41:40.252900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'endpoint': 'http://10.140.97.42',\n",
       " 'ak': 'OA8AZVFH6110XW4A51NX',\n",
       " 'sk': 'aZ5XfNpTZ8xSAa9lcs7MhYAy7wGr3WAzhzO5AfR8'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_s3_config(\"s3://private-crawl-data/zh-web-tencent/20241218_p1/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58a146641e22928a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:24:29.543180Z",
     "start_time": "2025-03-06T05:24:29.539608Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-06T10:41:40.257833Z",
     "iopub.status.busy": "2025-03-06T10:41:40.257689Z",
     "iopub.status.idle": "2025-03-06T10:41:40.286295Z",
     "shell.execute_reply": "2025-03-06T10:41:40.285629Z",
     "shell.execute_reply.started": "2025-03-06T10:41:40.257818Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "#安全解析函数（包含异常捕获）\n",
    "def safe_json_loads(s):\n",
    "    try:\n",
    "        json.loads(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "# 将Python函数注册为Spark UDF\n",
    "safe_json_udf = udf(safe_json_loads, BooleanType())\n",
    "\n",
    "# 在filter中使用UDF生成的Column表达式\n",
    "df_filtered = input_df.filter(safe_json_udf(col(\"value\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e160c-9f52-4d3c-aa2b-f1d0a4fe8c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5048ff51a7c6535",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:24:29.638602Z",
     "start_time": "2025-03-06T05:24:29.559110Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-06T10:41:45.815650Z",
     "iopub.status.busy": "2025-03-06T10:41:45.814946Z",
     "iopub.status.idle": "2025-03-06T10:41:45.917431Z",
     "shell.execute_reply": "2025-03-06T10:41:45.916623Z",
     "shell.execute_reply.started": "2025-03-06T10:41:45.815627Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "input_rdd = df_filtered.rdd.map(lambda x: Row(**{**json.loads(x.value), \"filename\": x.filename})).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20dfebb37369800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:24:29.658533Z",
     "start_time": "2025-03-06T05:24:29.656733Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ffc69dedbaef3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:24:29.686205Z",
     "start_time": "2025-03-06T05:24:29.674670Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-06T10:41:47.351808Z",
     "iopub.status.busy": "2025-03-06T10:41:47.351183Z",
     "iopub.status.idle": "2025-03-06T10:41:47.367094Z",
     "shell.execute_reply": "2025-03-06T10:41:47.366452Z",
     "shell.execute_reply.started": "2025-03-06T10:41:47.351787Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def transform_row(row, config: dict) -> Row:\n",
    "    \"\"\"根据平台配置转换行数据\"\"\"\n",
    "    mappings = config\n",
    "\n",
    "    return Row(\n",
    "        track_id=getattr(row, mappings[\"track_id\"],str(uuid.uuid4())),\n",
    "        url=getattr(row, mappings[\"url\"], ''),\n",
    "        html=getattr(row, mappings[\"html\"], ''),\n",
    "        page_layout_type=mappings.get(\"page_layout_type_map\").get(\n",
    "            getattr(row, mappings[\"layout_field\"], ''),\n",
    "            \"article\"\n",
    "        ),\n",
    "        domain=extract_domain_info(getattr(row, mappings[\"url\"], ''))['domain'],\n",
    "        dataset_name=mappings[\"dataset_name\"],\n",
    "        data_source_category=mappings[\"data_source_category\"],\n",
    "        meta_info={\"filename\":row.filename}\n",
    "    )\n",
    "#  pandas_df = input_rdd.toDF().toPandas()\n",
    "#  print(pandas_df)\n",
    "\n",
    "# 数据转换（仅用字段映射）\n",
    "transformed_rdd = input_rdd.map(\n",
    "    lambda row: transform_row(row, broadcast_field_mappings.value)\n",
    ").repartition(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3288a2f64143614e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:24:29.704007Z",
     "start_time": "2025-03-06T05:24:29.701822Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d20ce0eeef350ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:24:29.739740Z",
     "start_time": "2025-03-06T05:24:29.733719Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-06T10:41:53.570798Z",
     "iopub.status.busy": "2025-03-06T10:41:53.570253Z",
     "iopub.status.idle": "2025-03-06T10:41:53.578899Z",
     "shell.execute_reply": "2025-03-06T10:41:53.578297Z",
     "shell.execute_reply.started": "2025-03-06T10:41:53.570778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[16] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformed_rdd_1 = transformed_rdd.filter(lambda x: \"<!--VIDEO_0-->\"  not in x.html)\n",
    "# transformed_rdd_2 = transformed_rdd_1.filter(lambda x: \"<!--MUSIC_0-->\" not in x.html)\n",
    "# transformed_rdd_2 = transformed_rdd_2.cache()\n",
    "transformed_rdd.cache()\n",
    "# 18660\n",
    "# 62\n",
    "# 50000-(18660+62)=31278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c622407608220a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:24:29.784400Z",
     "start_time": "2025-03-06T05:24:29.779275Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-06T10:41:54.391601Z",
     "iopub.status.busy": "2025-03-06T10:41:54.390983Z",
     "iopub.status.idle": "2025-03-06T10:41:54.397357Z",
     "shell.execute_reply": "2025-03-06T10:41:54.396778Z",
     "shell.execute_reply.started": "2025-03-06T10:41:54.391579Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_data(partition, broadcast_extractor_config_path):\n",
    "   \n",
    "    from loguru import logger\n",
    "\n",
    "    extractor_chain = ExtractSimpleFactory.create('/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc')\n",
    "    timeout_seconds = 10\n",
    "    # 为每个分区创建唯一的错误日志文件\n",
    "    # partition_id = str(uuid.uuid4())\n",
    "    # current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # error_log_path = f\"s3://xyz-llm-users/xyz-users/yujia/CC-MAIN-2024-33/output/v002/error_logs/{current_time}_{partition_id}.json\"\n",
    "    # s3_doc_writer = S3DocWriter(path=error_log_path)\n",
    "    for row in partition:\n",
    "        try:\n",
    "\n",
    "            d = row.asDict()\n",
    "            input_data = DataJson(d)\n",
    "            data_e: DataJson = func_timeout(timeout_seconds, extractor_chain.extract, args=(input_data,))\n",
    "            #data_e: DataJson = extractor_chain.extract(input_data)\n",
    "            \n",
    "    \n",
    "            yield Row(**data_e.to_dict())\n",
    "        except FunctionTimedOut as e1:\n",
    "                d['__error'] = {\n",
    "                    \"error_type\":\"TIMEOUT\",\n",
    "                    \"error_message\": \"extract function timeout\",\n",
    "                    \"traceback\":\"TIMEOUT\"\n",
    "                }\n",
    "\n",
    "                yield Row(**d)\n",
    "        except Exception as e:\n",
    "                # 记录更详细的错误信息\n",
    "                error_info = {\n",
    "                    \"error_data\":Row(**d),\n",
    "                    \"error_type\": type(e).__name__,\n",
    "                    \"error_message\": str(e),\n",
    "                    \"traceback\": traceback.format_exc(),\n",
    "                }\n",
    "                logger.error(error_info)\n",
    "                # s3_doc_writer.write(error_info)\n",
    "                d['__error'] = error_info\n",
    "\n",
    "                yield Row(**d)\n",
    "           \n",
    "\n",
    "\n",
    "# 数据抽取（仅用抽取器配置路径）\n",
    "processed_rdd = transformed_rdd.mapPartitions(\n",
    "     lambda x:extract_data(\n",
    "        x,broadcast_extractor_config_path = broadcast_extractor_config_path.value\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302a8b6ac1f3cc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:24:29.801861Z",
     "start_time": "2025-03-06T05:24:29.800051Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44c9fedce6dedc3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:26:42.195011Z",
     "start_time": "2025-03-06T05:24:29.816851Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-06T10:41:56.775532Z",
     "iopub.status.busy": "2025-03-06T10:41:56.774883Z",
     "iopub.status.idle": "2025-03-06T10:54:24.685714Z",
     "shell.execute_reply": "2025-03-06T10:54:24.685022Z",
     "shell.execute_reply.started": "2025-03-06T10:41:56.775512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'field_mappings': {'track_id': 'track_id',\n",
       "  'page_layout_type_map': {'': 'article',\n",
       "   '文章': 'article',\n",
       "   '网易': 'article',\n",
       "   '视频': 'video',\n",
       "   '腾讯网': 'article'},\n",
       "  'dataset_name': 'tencent',\n",
       "  'url': 'url',\n",
       "  'html': 'content',\n",
       "  'layout_field': 'f_name',\n",
       "  'data_source_category': 'HTML'},\n",
       " 'extractor_config': '/share/jiangwenhao/notebooks/定向子集专项/subset-spliz.jsonc',\n",
       " 'output_template': 's3://llm-users-phdd2/jiangwenhao/article/zh-web-tencent/v018/'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = processed_rdd.map(lambda x: Row(value=json.dumps(x.asDict()))).filter(lambda x:\"__error\" not in x).toDF()\n",
    "# 输出结果Row(value=json.dumps(x.asDict()))).toDF()\n",
    "#write_any_path(processed_rdd.map(lambda x: Row(value=json.dumps(x.asDict()))).toDF(), platform_config[\"output_template\"],config)\n",
    "\n",
    "platform_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a1c66b2135477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:26:42.230692Z",
     "start_time": "2025-03-06T05:26:42.228326Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da373fa4169f9ad0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-06T05:26:47.728677Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-06T10:54:24.687441Z",
     "iopub.status.busy": "2025-03-06T10:54:24.687071Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:====================>                               (400 + 600) / 1000]"
     ]
    }
   ],
   "source": [
    "write_any_path(result_df, platform_config[\"output_template\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e57fe30189ebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:17:40.263694Z",
     "start_time": "2025-03-06T05:17:40.239571Z"
    }
   },
   "outputs": [],
   "source": [
    "x = \"s3://llm-users-phdd2/jiangwenhao/article/zh-web-tencent/v018/\"\n",
    "s3 = get_s3_client(x)\n",
    "_ = list(list_s3_objects(client=s3, path = x))\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "116c810a8e0db0e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:17:29.461396Z",
     "start_time": "2025-03-06T05:17:29.453998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'endpoint': 'http://10.135.0.241',\n",
       " 'ak': 'L5F6OE3EQEK00V4MV0E4',\n",
       " 'sk': 'oAwAZurrQps6VbOXKHpG9XjRtmjAG7ROOjfzquwC'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_s3_config(\"s3://llm-users-phdd2/jiangwenhao/article/zh-web-tencent/2019/v017/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ef42777197364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理广播变量\n",
    "broadcast_field_mappings.unpersist()\n",
    "broadcast_extractor_config_path.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbf2f91f-0df7-4cde-b72d-f2eae0f2c7b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T09:10:14.768501Z",
     "iopub.status.busy": "2025-02-28T09:10:14.767976Z",
     "iopub.status.idle": "2025-02-28T09:10:16.939031Z",
     "shell.execute_reply": "2025-02-28T09:10:16.938191Z",
     "shell.execute_reply.started": "2025-02-28T09:10:14.768479Z"
    }
   },
   "outputs": [],
   "source": [
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "\n",
    "from xinghe.s3 import *\n",
    "\n",
    "delete_path = \"s3://llm-users-phdd2/jiangwenhao/\"\n",
    "client = get_s3_client(delete_path)\n",
    "delete_s3_object(delete_path +'article/zh-web-baijiahao/v004', client = client, dry_run = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d579e28-d84f-406f-888a-c573e1fbbcf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T09:10:18.085813Z",
     "iopub.status.busy": "2025-02-28T09:10:18.085275Z",
     "iopub.status.idle": "2025-02-28T09:10:18.104946Z",
     "shell.execute_reply": "2025-02-28T09:10:18.104365Z",
     "shell.execute_reply.started": "2025-02-28T09:10:18.085792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://llm-users-phdd2/jiangwenhao/article/zh-web-baijiahao/v002/',\n",
       " 's3://llm-users-phdd2/jiangwenhao/article/zh-web-baijiahao/v003/',\n",
       " 's3://llm-users-phdd2/jiangwenhao/article/zh-web-baijiahao/v004/',\n",
       " 's3://llm-users-phdd2/jiangwenhao/article/zh-web-baijiahao/v005/']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list_s3_objects('s3://llm-users-phdd2/jiangwenhao/article/zh-web-baijiahao/', client = client, recursive=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20188a73-a567-48c9-94bf-df1496a1320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "   def process_platform_data(self, platform: str, input_paths: list, version: str = \"001\"):\n",
    "        \"\"\"处理指定平台数据的主流程\"\"\"\n",
    "        # 获取平台配置\n",
    "        config = self.platform_configs.get(platform)\n",
    "        if not config:\n",
    "            raise ValueError(f\"Unsupported platform: {platform}\")\n",
    "\n",
    "        # 读取原始数据\n",
    "        input_df = read_any_path(self.spark, \",\".join(input_paths), config)\n",
    "\n",
    "        # 数据转换\n",
    "        transformed_rdd = input_df.rdd.map(\n",
    "            partial(self._transform_row, platform=platform)\n",
    "        ).repartition(6000)\n",
    "\n",
    "        # 数据抽取\n",
    "        processed_rdd = transformed_rdd.mapPartitions(\n",
    "            partial(self._extract_data, platform=platform)\n",
    "        )\n",
    "\n",
    "        # 写入输出\n",
    "        output_path = config[\"output_template\"].format(\n",
    "            platform=platform,\n",
    "            version=version.zfill(3)\n",
    "        )\n",
    "        write_any_path(\n",
    "            processed_rdd.map(lambda x: Row(value=json.dumps(x))).toDF(),\n",
    "            output_path,\n",
    "            {\"skip_output_check\": True}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a02fe0-ef2c-490a-8c9a-8754ee35b9dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T06:57:42.315451Z",
     "iopub.status.busy": "2025-02-27T06:57:42.314884Z",
     "iopub.status.idle": "2025-02-27T06:57:48.535359Z",
     "shell.execute_reply": "2025-02-27T06:57:48.534021Z",
     "shell.execute_reply.started": "2025-02-27T06:57:42.315411Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SparkSession\n",
    "from typing import Dict, Any\n",
    "import json\n",
    "import uuid\n",
    "from functools import partial\n",
    "from typing import Iterable, Dict, Any\n",
    "# create spark session\n",
    "from pyspark.sql import Row, DataFrame\n",
    "\n",
    "from xinghe.spark import *\n",
    "from app.common.json_util import *\n",
    "\n",
    "from xinghe.s3 import *\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_4\",\n",
    "    \"skip_success_check\": True,\n",
    "    # 根据个人路径进行替换1\n",
    "    \"spark.executorEnv.LLM_WEB_KIT_CFG_PATH\": \"/share/jiangwenhao/.llm-web-kit.jsonc\",\n",
    "    \"spark.dynamicAllocation.maxExecutors\": \"400\",\n",
    "}\n",
    "\n",
    "spark = new_spark_session(\"llm_kit_cc\", config)\n",
    "\n",
    "processor = WebDataProcessor(spark)\n",
    "platform=\"netease\"\n",
    "input_paths=[\"s3://private-crawl-data/zh-web-netease/20241218_p1/\"]\n",
    "version=\"002\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f2b69ea-917f-4332-91ba-b367fc4076d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T06:58:27.000191Z",
     "iopub.status.busy": "2025-02-27T06:58:26.999496Z",
     "iopub.status.idle": "2025-02-27T06:58:35.020208Z",
     "shell.execute_reply": "2025-02-27T06:58:35.019375Z",
     "shell.execute_reply.started": "2025-02-27T06:58:27.000164Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 获取平台配置\n",
    "self =processor\n",
    "config = processor.platform_configs.get(platform)\n",
    "if not config:\n",
    "    raise ValueError(f\"Unsupported platform: {platform}\")\n",
    "\n",
    "# 读取原始数据\n",
    "input_df = read_any_path(self.spark, \",\".join(input_paths), config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3016239c-25a0-4a75-9cc4-8d8a96f51fc3",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-02-27T07:04:56.900227Z",
     "iopub.status.busy": "2025-02-27T07:04:56.899721Z",
     "iopub.status.idle": "2025-02-27T07:05:48.300570Z",
     "shell.execute_reply": "2025-02-27T07:05:48.299631Z",
     "shell.execute_reply.started": "2025-02-27T07:04:56.900203Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/27 15:05:42 WARN TaskSetManager: Lost task 120.0 in stage 13.0 (TID 694) (host-10-140-92-29 executor 1): java.io.EOFException: Unexpected end of input stream\n",
      "\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:165)\n",
      "\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n",
      "\tat java.io.InputStream.read(InputStream.java:101)\n",
      "\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "25/02/27 15:05:48 ERROR TaskSetManager: Task 120 in stage 13.0 failed 4 times; aborting job\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 119.0 in stage 13.0 (TID 693) (host-10-140-92-29 executor 1): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 117.0 in stage 13.0 (TID 691) (host-10-140-93-30 executor 4): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 112.0 in stage 13.0 (TID 686) (host-10-140-93-30 executor 4): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 113.0 in stage 13.0 (TID 687) (host-10-140-93-30 executor 4): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 111.0 in stage 13.0 (TID 685) (host-10-140-93-30 executor 4): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 110.0 in stage 13.0 (TID 684) (host-10-140-93-30 executor 4): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 114.0 in stage 13.0 (TID 688) (host-10-140-93-30 executor 4): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 116.0 in stage 13.0 (TID 690) (host-10-140-93-30 executor 4): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 115.0 in stage 13.0 (TID 689) (host-10-140-93-30 executor 4): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 100.0 in stage 13.0 (TID 674) (host-10-140-93-30 executor 5): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 105.0 in stage 13.0 (TID 679) (host-10-140-93-30 executor 5): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 103.0 in stage 13.0 (TID 677) (host-10-140-93-30 executor 5): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 109.0 in stage 13.0 (TID 683) (host-10-140-93-30 executor 5): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 101.0 in stage 13.0 (TID 675) (host-10-140-93-30 executor 5): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 107.0 in stage 13.0 (TID 681) (host-10-140-93-30 executor 5): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 104.0 in stage 13.0 (TID 678) (host-10-140-93-30 executor 5): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 106.0 in stage 13.0 (TID 680) (host-10-140-93-30 executor 5): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 102.0 in stage 13.0 (TID 676) (host-10-140-93-30 executor 5): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 91.0 in stage 13.0 (TID 665) (host-10-140-93-30 executor 6): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 95.0 in stage 13.0 (TID 669) (host-10-140-93-30 executor 6): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 99.0 in stage 13.0 (TID 673) (host-10-140-93-30 executor 6): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 90.0 in stage 13.0 (TID 664) (host-10-140-93-30 executor 6): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 98.0 in stage 13.0 (TID 672) (host-10-140-93-30 executor 6): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 96.0 in stage 13.0 (TID 670) (host-10-140-93-30 executor 6): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 97.0 in stage 13.0 (TID 671) (host-10-140-93-30 executor 6): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 118.0 in stage 13.0 (TID 692) (host-10-140-93-30 executor 4): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 93.0 in stage 13.0 (TID 667) (host-10-140-93-30 executor 6): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 94.0 in stage 13.0 (TID 668) (host-10-140-93-30 executor 6): TaskKilled (Stage cancelled)\n",
      "25/02/27 15:05:48 WARN TaskSetManager: Lost task 92.0 in stage 13.0 (TID 666) (host-10-140-93-30 executor 6): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1376.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 120 in stage 13.0 failed 4 times, most recent failure: Lost task 120.3 in stage 13.0 (TID 697) (host-10-140-92-130 executor 2): java.io.EOFException: Unexpected end of input stream\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:165)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n\tat java.io.InputStream.read(InputStream.java:101)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.EOFException: Unexpected end of input stream\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:165)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n\tat java.io.InputStream.read(InputStream.java:101)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minput_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/envs/code_clean_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/envs/code_clean_venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1376.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 120 in stage 13.0 failed 4 times, most recent failure: Lost task 120.3 in stage 13.0 (TID 697) (host-10-140-92-130 executor 2): java.io.EOFException: Unexpected end of input stream\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:165)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n\tat java.io.InputStream.read(InputStream.java:101)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.EOFException: Unexpected end of input stream\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:165)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n\tat java.io.InputStream.read(InputStream.java:101)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:200)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "input_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae0f8039-0abe-4904-a294-41e41964e6fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T07:06:29.042381Z",
     "iopub.status.busy": "2025-02-27T07:06:29.041877Z",
     "iopub.status.idle": "2025-02-27T07:06:29.048494Z",
     "shell.execute_reply": "2025-02-27T07:06:29.047939Z",
     "shell.execute_reply.started": "2025-02-27T07:06:29.042358Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_fraction = 0.1\n",
    "if sample_fraction==1.0:\n",
    "    df_sample = input_df\n",
    "else:\n",
    "    df_sample = input_df.sample(fraction=sample_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bece73a-edd6-45fc-bebe-3eca0929fedb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T07:08:11.726527Z",
     "iopub.status.busy": "2025-02-27T07:08:11.726012Z",
     "iopub.status.idle": "2025-02-27T07:08:11.734047Z",
     "shell.execute_reply": "2025-02-27T07:08:11.733525Z",
     "shell.execute_reply.started": "2025-02-27T07:08:11.726504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = df_sample.limit(1)\n",
    "type(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1d15366-8ca4-4078-afef-af42e5da0d26",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-02-27T07:09:16.077057Z",
     "iopub.status.busy": "2025-02-27T07:09:16.076486Z",
     "iopub.status.idle": "2025-02-27T07:09:16.743765Z",
     "shell.execute_reply": "2025-02-27T07:09:16.742724Z",
     "shell.execute_reply.started": "2025-02-27T07:09:16.077034Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/27 15:09:16 WARN TaskSetManager: Lost task 0.0 in stage 33.0 (TID 1197) (host-10-140-92-130 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/worker.py\", line 820, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 828, in func\n",
      "    return f(iterator)\n",
      "  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2297, in <lambda>\n",
      "    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
      "  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2297, in <genexpr>\n",
      "    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
      "  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "TypeError: WebDataProcessor._transform_row() missing 1 required positional argument: 'row'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "25/02/27 15:09:16 ERROR TaskSetManager: Task 0 in stage 33.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 4 times, most recent failure: Lost task 0.3 in stage 33.0 (TID 1200) (host-10-140-92-130 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/worker.py\", line 820, in process\n    out_iter = func(split_index, iterator)\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 828, in func\n    return f(iterator)\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2297, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2297, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\nTypeError: WebDataProcessor._transform_row() missing 1 required positional argument: 'row'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/worker.py\", line 820, in process\n    out_iter = func(split_index, iterator)\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 828, in func\n    return f(iterator)\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2297, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2297, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\nTypeError: WebDataProcessor._transform_row() missing 1 required positional argument: 'row'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m transformed_rdd \u001b[38;5;241m=\u001b[39m df_sample\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m      2\u001b[0m         partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_row, platform\u001b[38;5;241m=\u001b[39mplatform)\n\u001b[1;32m      3\u001b[0m     )\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtransformed_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py:2297\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2279\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   2296\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py:2272\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[NumberOrArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumberOrArray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2252\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2253\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2254\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   2273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\n\u001b[1;32m   2274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py:2025\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   2020\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   2023\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 2025\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m~/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1814\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/envs/code_clean_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/envs/code_clean_venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 4 times, most recent failure: Lost task 0.3 in stage 33.0 (TID 1200) (host-10-140-92-130 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/worker.py\", line 820, in process\n    out_iter = func(split_index, iterator)\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 828, in func\n    return f(iterator)\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2297, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2297, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\nTypeError: WebDataProcessor._transform_row() missing 1 required positional argument: 'row'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/worker.py\", line 820, in process\n    out_iter = func(split_index, iterator)\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 828, in func\n    return f(iterator)\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2297, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/share/jiangwenhao/envs/code_clean_venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2297, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/data/nm-local-dir/usercache/jiangwenhao/appcache/application_1738517174793_0504/container_e02_1738517174793_0504_01_000003/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\nTypeError: WebDataProcessor._transform_row() missing 1 required positional argument: 'row'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "transformed_rdd = df_sample.rdd.map(\n",
    "        partial(self._transform_row, platform=platform)\n",
    "    )\n",
    "transformed_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427e7c1-a4ae-4daa-9b66-f6a0d95abdd1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-27T06:57:14.588402Z",
     "iopub.status.idle": "2025-02-27T06:57:14.588583Z",
     "shell.execute_reply": "2025-02-27T06:57:14.588499Z",
     "shell.execute_reply.started": "2025-02-27T06:57:14.588490Z"
    }
   },
   "outputs": [],
   "source": [
    "processor.platform_configs.get(platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a1d4f-46ab-4827-9196-c294bba17cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2871eb-688c-4b43-aaf6-9ac5dc7b95d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c755fc-9c00-4570-882e-0e296176f258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d379d8c-8e47-4b9c-ad8b-ef45f75e8cff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee7d4d-c2c0-4416-9f14-a78f9aa58bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574cfe45-8e5c-4c50-8404-80d47bbd3815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1a26b-a640-4cd5-a40c-1994935aa4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0914a8-dd82-4f98-8428-b769dd81a6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fefd6a1-8976-4b68-9754-1371d59e512d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python code_clean_venv (ipykernel)",
   "language": "python",
   "name": "test_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
