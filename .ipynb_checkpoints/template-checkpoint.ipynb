{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a622437",
   "metadata": {},
   "source": [
    "**数据查看工具**\n",
    "\n",
    "可使用 [S3_Browser](http://10.140.0.104:10001/S3_Browser) 查看对象存储上的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eecb34",
   "metadata": {},
   "source": [
    "**选择Spark集群**\n",
    "\n",
    "`spark_conf_name` 有8个可选值，分别为 `spark_[1234](_old)?`\n",
    "\n",
    "集群概况如下，点击集群名字，可查看各集群的当前资源使用情况：\n",
    "\n",
    "- [spark_1](http://10.140.0.100:8088/cluster/scheduler): 共约30节点，与S集群Ceph有高速网络，CPU主频高，内存高。\n",
    "- [spark_2](http://10.140.84.107:8088/cluster/scheduler): 共96节点，与P、T集群对象存储有高速网络，CPU核数多，内存少。\n",
    "- [spark_3](http://10.140.84.31:8088/cluster/scheduler): 共40节点，与P、T集群对象存储有高速网络，CPU核数多，内存少，有常驻任务。\n",
    "- [spark_4](http://10.140.52.131:8088/cluster/scheduler): 共44节点，与任何对象存储都只有**低速**网络，CPU核数多，内存少，不支持需要Shuffle操作的任务。\n",
    "\n",
    "其他说明：\n",
    "\n",
    "- Spark版本默认为较新的 `3.4.x` ，若需使用 `2.4.x` 版本，请添加 `_old` 后缀。\n",
    "- Spark `2.x` 版本，支持的最高的python版本为 `3.7` ，若选择使用Spark `2.x` ，请切换Notebook的Kernel至 `3.7` 版本。\n",
    "- 执行 `spark` ，将spark对象输出到Output中，可获得当前Spark Session的UI链接，打开UI可了解作业的执行情况，打开之前请 [配置Hosts](https://aicarrier.feishu.cn/wiki/R4uDwvptPiScaXkJMJvcn8AOnCg) 。\n",
    "- Spark所能读取的对象存储桶名中，只能包含英文字母，数字和短横线(-)，不能包含下划线(\\_)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4fc20",
   "metadata": {},
   "source": [
    "**Notebook Kernel的选择**\n",
    "\n",
    "- 若使用Spark 3.x (不带 `_old` 后缀的集群名)，请使用Pythen 3.10 Kernel。\n",
    "- 若使用Spark 2.x (即带 `_old` 后缀的集群名)，请使用Pythen 3.7 Kernel。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48841bf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create spark session\n",
    "from pyspark.sql import Row, DataFrame\n",
    "\n",
    "from app.common.spark_ext import *\n",
    "from app.common.json_util import *\n",
    "from app.common.s3 import *\n",
    "\n",
    "config = {\n",
    "    \"spark_conf_name\": \"spark_1\",\n",
    "    \"skip_success_check\": True,\n",
    "}\n",
    "\n",
    "spark = new_spark_session(\"changeit\", config)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "input_paths = [\n",
    "    \"s3://BUCKET/path/to/input/dir/\",\n",
    "]\n",
    "input_df = read_any_path(spark, \",\".join(input_paths), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view data\n",
    "input_df.printSchema()\n",
    "input_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb54a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "def filter_data(row):\n",
    "    return json_loads(row.value)[\"field\"] == \"expected\"\n",
    "\n",
    "output_df = input_df.rdd.filter(filter_data).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data\n",
    "output_path = \"s3://BUCKET/path/to/output/dir/\"\n",
    "write_any_path(output_df, output_path, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ipykernel)",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
